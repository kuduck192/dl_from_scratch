{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 100) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from einops import rearrange, einsum\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "M, N = 5000, 100\n",
    "X, y = make_regression(M, N)\n",
    "y = ((y-y.mean())/abs(y-y.mean()).max()).reshape(-1, 1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation='relu'):\n",
    "    if activation == 'relu':\n",
    "        def relu(x): return 0 + x * (x > 0).astype(int)\n",
    "        return relu\n",
    "    \n",
    "def get_grad_fn(activation='relu'):\n",
    "    if activation == 'relu':\n",
    "        def grad_relu(z): return (z >= 0).astype(np.float32)\n",
    "        return grad_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, einsum\n",
    "\n",
    "\n",
    "def xavier_uniform(*shape):\n",
    "    assert len(shape) >= 2\n",
    "    in_fan, out_fan = shape[-2], shape[-1]\n",
    "    ret =  2 * (np.random.rand(*shape) - 0.5)\n",
    "    ret *= (6 / (in_fan + out_fan)) ** 0.5\n",
    "    return ret\n",
    "    \n",
    "\n",
    "class MLPRegressor():\n",
    "    def __init__(self, hidden_layer_sizes=(100,), solver='sgd', activation='relu', lr=1e-3, max_iters=200, optim_kwargs={}):\n",
    "        self.max_iters = max_iters\n",
    "        self.lr = lr\n",
    "        assert solver in ['sgd'], \"Solver is not supported yet.\"\n",
    "        assert activation in ['identity', 'logistic', 'tanh', 'relu'], \"Activation is not supported yet.\"\n",
    "        self.activation = activation\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optim_kwargs = optim_kwargs\n",
    "        self.act_fn = get_activation_fn(activation)\n",
    "        self.grad_fn = get_grad_fn(activation)\n",
    "        self.eps = 1e-4\n",
    "        \n",
    "    def _init_optimizer(self):\n",
    "        if self.solver == 'sgd':\n",
    "            self.optimizer = SGD(**self.optim_kwargs)\n",
    "        if self.solver == 'adam':\n",
    "            self.optimizer = Adam(**self.optim_kwargs)\n",
    "        \n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        prev_size = self.input_size\n",
    "        for hidden_layer_size in self.hidden_layer_sizes:\n",
    "            self.weights.append(xavier_uniform(hidden_layer_size, prev_size))\n",
    "            self.biases.append(np.array(0.0))\n",
    "            prev_size = hidden_layer_size\n",
    "        self.weights.append(xavier_uniform(1, prev_size))\n",
    "        self.biases.append(np.array(0.0))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        M, N = X.shape\n",
    "        \n",
    "        self.input_size = N\n",
    "        self._init_weights()\n",
    "        y = y.reshape(M, 1)\n",
    "        \n",
    "        \n",
    "        prev_loss = float('inf')\n",
    "        for iteration in range(self.max_iters):\n",
    "            grad_weights, grad_biases = self.calc_grad(X, y)\n",
    "            for layer_idx in range(len(self.weights)):\n",
    "                self.weights[layer_idx] -= grad_weights[layer_idx] * self.lr\n",
    "                self.biases[layer_idx] -= grad_biases[layer_idx] * self.lr\n",
    "            \n",
    "            if (iteration + 1) % (self.max_iters // 10) == 0:\n",
    "                print(prev_loss)\n",
    "            curr_loss = self.calc_loss(X, y)\n",
    "            if curr_loss < prev_loss - self.eps:\n",
    "                prev_loss = curr_loss\n",
    "            else:\n",
    "                break\n",
    "              \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        Zs = [X.copy()]\n",
    "        As = [X.copy()]\n",
    "        L = len(self.weights)\n",
    "        for layer_idx in range(L - 1):\n",
    "            Z = As[-1].copy() @ self.weights[layer_idx].T + self.biases[layer_idx]\n",
    "            Zs.append(Z)\n",
    "            As.append(self.act_fn(Z))\n",
    "            \n",
    "        y_pred = As[-1] @ self.weights[-1].T + self.biases[-1]\n",
    "        return y_pred\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return ((y - y_pred) ** 2).mean()\n",
    "        \n",
    "    def calc_grad(self, X, y):\n",
    "        Zs = [X.copy()]\n",
    "        As = [X.copy()]\n",
    "        L = len(self.weights)\n",
    "        for layer_idx in range(L - 1):\n",
    "            Z = As[-1].copy() @ self.weights[layer_idx].T + self.biases[layer_idx]\n",
    "            Zs.append(Z)\n",
    "            As.append(self.act_fn(Z))\n",
    "        y_pred = As[-1] @ self.weights[-1].T + self.biases[-1]\n",
    "        self.Zs = Zs\n",
    "        self.As = As\n",
    "        \n",
    "        E = (y_pred - y)\n",
    "        dL_dWlast = 2 * einsum(As[-1], E, 'm a, m b -> m a b').mean(axis=0)\n",
    "        dL_dblast = 2 * E.mean(axis=0).sum()\n",
    "        gradW_list = []\n",
    "        gradb_list = []\n",
    "        gradW_list.append(dL_dWlast.T)\n",
    "        gradb_list.append(dL_dblast)\n",
    "        leading_term = self.weights[-1].T[None].repeat(M, axis=0)\n",
    "        L = len(self.weights)\n",
    "        for layer_idx in range(L - 1, 0, -1):\n",
    "          Z = Zs[layer_idx]\n",
    "          A = As[layer_idx]\n",
    "          Z_prev = Zs[layer_idx - 1]\n",
    "          A_prev = As[layer_idx - 1]\n",
    "          dA_dW = einsum(self.grad_fn(Z), A_prev, 'm b, m a -> m a b')\n",
    "          dE_dW = einsum(leading_term, dA_dW, 'm b c, m a b -> m a b c')\n",
    "          dL_dW = 2 * einsum(dE_dW, E, 'm a b c, m c -> m a b c').mean(axis=0).mean(axis=-1)\n",
    "          gradW = dL_dW.T\n",
    "          gradW_list.append(gradW)\n",
    "          \n",
    "          dA_db = self.grad_fn(Z)\n",
    "          dE_db = einsum(leading_term, dA_db, 'm b c, m b -> m b c')\n",
    "          dL_db = 2 * einsum(dE_db, E, 'm b c, m c -> m b c').mean(axis=0).sum()\n",
    "          gradb = dL_db\n",
    "          gradb_list.append(gradb)\n",
    "\n",
    "          if layer_idx > 1:\n",
    "            dA_dA = einsum(self.grad_fn(Z), self.weights[layer_idx - 1].T, 'm b, a b -> m a b')\n",
    "            leading_term = einsum(leading_term, dA_dA, 'm b c, m a b -> m a c')\n",
    "    \n",
    "        return gradW_list[::-1], gradb_list[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2960851363444067\n",
      "0.23675113324494682\n",
      "0.20076168018480675\n",
      "0.17651492814149097\n",
      "0.159126974756254\n",
      "0.14610521642505178\n",
      "0.136028142647592\n",
      "0.12799087408862556\n",
      "0.12140896949094176\n",
      "0.11592111980319915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11542439570657036"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 64), max_iters=100, lr=1e-2).fit(X, y)\n",
    "mlp.calc_loss(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
